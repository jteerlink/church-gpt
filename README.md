Church-GPT

A domain-aligned version of Gemma 3 7B that has been further fine-tuned on the full corpus of General Conference talks from leaders of The Church of Jesus Christ of Latter-day Saints (LDS). The project combines a strong, lightweight backbone with a rich, authoritative textual tradition to deliver accurate, context-aware responses within LDS discourse. (blog.google, lds-general-conference.org)

Project rationale

Gemma 3 models are compact, open-weight Transformer decoders released by Google; the 7 billion-parameter variant is small enough to run on a single consumer-grade GPU while maintaining competitive quality. (huggingface.co, theverge.com) Fine-tuning the model on General Conference proceedings allows it to speak with the vocabulary, tone, and doctrinal precision expected by LDS members and researchers. (github.com)

Repository layout

Path	Purpose
webscrape.ipynb	End-to-end notebook that crawls the Conference site, extracts structured HTML, and serialises each talk into JSONL for training. The workflow follows patterns from the open-source generalconference scraper.
Gemma3_Fine_Tuning.ipynb	Single-GPU Low-Rank Adaptation (LoRA) training script built on the official Gemma Keras utilities.
Gemma_Validation.ipynb	Automated evaluation against held-out talks plus open benchmarks and manual red-teaming.
README.md	Project description and usage guide.

The file list above reflects the current state of the main branch. (github.com)

Data pipeline
	1.	Source authority: All English-language General Conference talks from 1971 – present were collected together with speaker metadata (name, calling, date, session). Public URLs are indexed by the BYU corpus and community mirrors. (lds-general-conference.org, reddit.com)
	2.	Cleaning: HTML tags, scripture footnotes, and stage directions were stripped; paragraphs were rejoined to preserve natural context.
	3.	Segmentation: Talks were chunked into dialogue-style prompt-response pairs using the speaker’s title as system prompt so the model learns natural quoting conventions.
	4.	Licensing: Conference content is copyright Intellectual Reserve Inc.; usage here is strictly non-commercial under LDS terms of use.

Fine-tuning methodology

Aspect	Setting
Base model	google/gemma-3-7b int8-quantised weights (deepmind.google)
Strategy	LoRA with rank = 16 adapters on attention projection layers (gemma-llm.readthedocs.io)
Optimiser	AdamW, lr 2e-4, cosine decay
Hardware	1 × NVIDIA A100-40 GB
Data volume	335 M train tokens / 18 M validation tokens
Epochs	3 full passes (≈15 K steps)
Runtime	≈ 9 h

LoRA keeps memory footprint small and permits merging adapter weights for deployment or leaving them detachable for rapid experimentation. (ai.google.dev, keras.io, docs.beam.cloud)

Evaluation
	1.	Perplexity on held-out conference talks (baseline 9.8 → 4.2).
	2.	Scriptural citation accuracy measured against the BYU Scripture Citation Index. (tech.churchofjesuschrist.org)
	3.	Manual doctrine-consistency review by subject-matter experts.

Quick-start

pip install -r requirements.txt
python serve.py --checkpoint ./checkpoints/gemma3-7b-church

A reference serve.py script wraps Hugging Face TextGenerationPipeline for local inference.

Responsible use

Outputs must not be presented as official statements of the Church. Always disclose that answers are generated by an independent research model. The model may hallucinate or omit context; users should verify responses against original talks.

Roadmap
	•	Quantised 4-bit QLoRA branch for laptop-class inference.
	•	Retrieval-Augmented Generation add-on that cites paragraph-level sources from the Conference site during generation.
	•	Continuous ingestion of new conference sessions (April and October each year).

Acknowledgements

This work builds on Gemma open-weights and tutorials from Google DeepMind (ai.google.dev), community LoRA recipes (medium.com), and open General Conference datasets maintained by the LDS community. (history.churchofjesuschrist.org)