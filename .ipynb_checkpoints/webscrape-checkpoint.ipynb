{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeed0d43-7449-40ab-8ea6-4d1572b80bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Google Drive integration\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4a1de12-9abd-4565-9fde-1138059f2b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "BASE_URL = \"https://www.churchofjesuschrist.org\"\n",
    "START_URL = f\"{BASE_URL}/study/general-conference?lang=eng\"\n",
    "GDRIVE_FOLDER_ID = \"YOUR_DRIVE_FOLDER_ID\"  # ← replace with your folder ID\n",
    "\n",
    "# 1. Authenticate with Google Drive\n",
    "gauth = GoogleAuth()\n",
    "gauth.LocalWebserverAuth()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dd3be19-df67-4499-a0ee-b4ee1af88289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "# Persistent HTTP session for connection pooling and cookies\n",
    "session = requests.Session()  # \n",
    "session.headers.update({\"User-Agent\": \"GC-Scraper/1.0\"})\n",
    "\n",
    "def get_conference_urls():\n",
    "    \"\"\"\n",
    "    Generate all April and October conference page URLs\n",
    "    from 1971 up to today, and return only those that exist.\n",
    "    \"\"\"\n",
    "    today = date.today()  # \n",
    "    urls = []\n",
    "    for year in range(1995, 2000):\n",
    "        for month in (1,2,3,5,6,7,8,9,11,12):\n",
    "            # Skip future sessions in the current year\n",
    "            if year == today.year and month > today.month:\n",
    "                continue\n",
    "            url = f\"{BASE_URL}/study/general-conference/{year}/{month:02d}?lang=eng\"\n",
    "            resp = session.head(url)  # \n",
    "            if resp.status_code == 200:\n",
    "                urls.append(url)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d1bfaf7-b749-4c3b-94da-7ad5cabca8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = get_conference_urls()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "852bce55-0a43-49cb-a70f-94ee75928a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.churchofjesuschrist.org/study/general-conference/1995/04?lang=eng',\n",
       " 'https://www.churchofjesuschrist.org/study/general-conference/1995/10?lang=eng',\n",
       " 'https://www.churchofjesuschrist.org/study/general-conference/1996/04?lang=eng',\n",
       " 'https://www.churchofjesuschrist.org/study/general-conference/1996/10?lang=eng',\n",
       " 'https://www.churchofjesuschrist.org/study/general-conference/1997/04?lang=eng',\n",
       " 'https://www.churchofjesuschrist.org/study/general-conference/1997/10?lang=eng',\n",
       " 'https://www.churchofjesuschrist.org/study/general-conference/1998/04?lang=eng',\n",
       " 'https://www.churchofjesuschrist.org/study/general-conference/1998/10?lang=eng',\n",
       " 'https://www.churchofjesuschrist.org/study/general-conference/1999/04?lang=eng',\n",
       " 'https://www.churchofjesuschrist.org/study/general-conference/1999/10?lang=eng']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8071addd-1919-4b43-84bd-ccf8f321822b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?client_id=722991589272-caqeipkkl4lseu15js5l3nfvl7pejiae.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&access_type=offline&response_type=code\n",
      "\n",
      "Authentication successful.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "\n",
    "GDRIVE_FOLDER_ID = \"1TYYeDa41R2FYXh5IC6YHONTaBIcp5kQY\"\n",
    "# Persistent HTTP session for connection pooling and cookies\n",
    "\n",
    "import time\n",
    "from requests.exceptions import SSLError, ReadTimeout\n",
    "\n",
    "def robust_get(url, **kwargs):\n",
    "    for attempt in range(10):\n",
    "        try:\n",
    "            return session.get(url, timeout=(5, 10), **kwargs)\n",
    "        except (SSLError, ReadTimeout) as e:\n",
    "            wait = 2 ** attempt\n",
    "            print(f\"Warning: {e.__class__.__name__} on {url}, retrying in {wait}s...\")\n",
    "            time.sleep(wait)\n",
    "    # last attempt, let exception propagate\n",
    "    return session.get(url, timeout=(5, 10), **kwargs)\n",
    "\n",
    "# Optional: if SSL certs are causing you grief (not recommended for production)\n",
    "# session.verify = False\n",
    "\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; GC-Scraper/1.0)\"\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "session = requests.Session()  # \n",
    "session.headers.update({\"User-Agent\": \"GC-Scraper/1.0\"})\n",
    "\n",
    "# Authenticate to Google Drive\n",
    "gauth = GoogleAuth()\n",
    "gauth.LocalWebserverAuth()  # \n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "def scrape_conference(conf_url, session):\n",
    "    \"\"\"\n",
    "    Return a sorted list of full talk URLs for a given conference page.\n",
    "    \"\"\"\n",
    "    resp = robust_get(conf_url)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    # 1. Find all <a> tags matching the talk pattern\n",
    "    anchors = soup.find_all(\n",
    "        \"a\",\n",
    "        href=re.compile(r\"^/study/general-conference/\\d{4}/\\d{2}/.+\\?lang=eng$\")\n",
    "    )\n",
    "\n",
    "    # 2. Normalize and collect unique URLs\n",
    "    links = set()\n",
    "    for a in anchors:\n",
    "        href = a[\"href\"]\n",
    "        if not href.startswith(\"http\"):\n",
    "            href = BASE_URL + href\n",
    "        links.add(href)\n",
    "\n",
    "    return sorted(links)\n",
    "\n",
    "def scrape_and_upload():\n",
    "    \"\"\"Main workflow: scrape all talks and mirror them into Google Drive.\"\"\"\n",
    "    os.makedirs(\"gc_texts\", exist_ok=True)  # \n",
    "\n",
    "    for conf_url in get_conference_urls():\n",
    "        year, month = re.search(r\"/(\\d{4})/(\\d{2})\\?lang=eng$\", conf_url).groups()\n",
    "        conf_dir = os.path.join(\"gc_texts\", f\"{year}-{month}\")\n",
    "        os.makedirs(conf_dir, exist_ok=True)\n",
    "\n",
    "        for talk_url in scrape_conference(conf_url, session):\n",
    "            # Extract talk slug for filenames\n",
    "            slug = talk_url.split(\"/\")[-1].replace(\"?lang=eng\", \"\")\n",
    "            page = robust_get(talk_url)\n",
    "            page.raise_for_status()\n",
    "            # Extract full visible text from <article> or fallback to whole page\n",
    "            text = (\n",
    "                BeautifulSoup(page.text, \"html.parser\")\n",
    "                .get_text(separator=\"\\n\", strip=True)  # \n",
    "            )\n",
    "            file_path = os.path.join(conf_dir, f\"{slug}.txt\")\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(text)\n",
    "\n",
    "            # Upload to Google Drive\n",
    "            gfile = drive.CreateFile({'parents': [{'id': GDRIVE_FOLDER_ID}]})  # \n",
    "            gfile.SetContentFile(file_path)  # \n",
    "            gfile.Upload()                   # \n",
    "            time.sleep(2)                    # \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_and_upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "780767a9-7814-4d75-bda4-30e164090371",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape liahona articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0e0beaf-c22d-4f7a-af10-9b8551fd4385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?client_id=722991589272-caqeipkkl4lseu15js5l3nfvl7pejiae.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&access_type=offline&response_type=code\n",
      "\n",
      "Authentication successful.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "from datetime import date  #  [oai_citation_attribution:9‡Python documentation](https://docs.python.org/3/library/datetime.html?utm_source=chatgpt.com)\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from bs4 import BeautifulSoup\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "\n",
    "# --- Configuration ---\n",
    "START_YEAR = 2008              # adjust based on the first Liahona issue you care about\n",
    "GDRIVE_FOLDER_ID = \"1TYYeDa41R2FYXh5IC6YHONTaBIcp5kQY\"\n",
    "BASE_URL = \"https://www.churchofjesuschrist.org\"\n",
    "LANG_PARAM = \"?lang=eng\"\n",
    "\n",
    "# --- 1. HTTP Session w/ Retries & Timeouts ---\n",
    "session = requests.Session()  #  [oai_citation_attribution:10‡Requests](https://requests.readthedocs.io/en/master/user/quickstart/?utm_source=chatgpt.com)\n",
    "retry_strategy = Retry(\n",
    "    total=5,\n",
    "    backoff_factor=1,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
    ")  #  [oai_citation_attribution:11‡urllib3](https://urllib3.readthedocs.io/en/stable/reference/urllib3.util.html?utm_source=chatgpt.com)\n",
    "\n",
    "adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "session.mount(\"https://\", adapter)\n",
    "session.mount(\"http://\", adapter)\n",
    "session.headers.update({\"User-Agent\": \"Liahona-Scraper/1.0\"})\n",
    "\n",
    "# --- 2. Google Drive Authentication ---\n",
    "gauth = GoogleAuth()\n",
    "gauth.LocalWebserverAuth()  # OAuth via local webserver  [oai_citation_attribution:12‡urllib3](https://urllib3.readthedocs.io/en/stable/reference/urllib3.util.html?utm_source=chatgpt.com)\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92fb6d68-635f-4d02-98c2-7def46e80c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_urls(start_year=START_YEAR):\n",
    "    \"\"\"Generate and return all existing Liahona issue URLs from start_year → today.\"\"\"\n",
    "    today = date.today()  #  [oai_citation_attribution:13‡Python documentation](https://docs.python.org/3/library/datetime.html?utm_source=chatgpt.com)\n",
    "    urls = []\n",
    "    for yr in range(start_year, today.year + 1):\n",
    "        for mo in (1,2,3,5,6,7,8,9,11,12):\n",
    "        # for mo in (8,9,11,12):\n",
    "            if yr == today.year and mo > today.month:\n",
    "                break\n",
    "            url = f\"{BASE_URL}/study/liahona/{yr}/{mo:02d}{LANG_PARAM}\"\n",
    "            try:\n",
    "                resp = session.head(url, timeout=(3, 7))  #  [oai_citation_attribution:14‡W3Schools.com](https://www.w3schools.com/Python/ref_requests_head.asp?utm_source=chatgpt.com)\n",
    "                if resp.status_code == 200:\n",
    "                    urls.append((yr, mo, url))\n",
    "            except requests.RequestException:\n",
    "                continue\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c620cd-5b64-4660-ba59-d0f6267ba3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2008-01\n",
      " → Uploaded a-missionary-in-the-making-john-kay-of-glenrothes-fife-scotland.txt\n"
     ]
    }
   ],
   "source": [
    "def scrape_month(yr, mo, url):\n",
    "    \"\"\"Return a sorted list of full article URLs for a given Liahona month page.\"\"\"\n",
    "    resp = session.get(url, timeout=(5, 15))\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")  #  [oai_citation_attribution:15‡Beautiful Soup Documentation](https://beautiful-soup-4.readthedocs.io/en/latest/?utm_source=chatgpt.com)\n",
    "\n",
    "    anchors = soup.find_all(\n",
    "        \"a\",\n",
    "        href=re.compile(rf\"^/study/liahona/{yr}/{mo:02d}/.+{re.escape(LANG_PARAM)}$\")\n",
    "    )  #  [oai_citation_attribution:16‡Python documentation](https://docs.python.org/3/library/re.html?utm_source=chatgpt.com)\n",
    "\n",
    "    links = {\n",
    "        (href if href.startswith(\"http\") else BASE_URL + href)\n",
    "        for a in anchors for href in [a[\"href\"]]\n",
    "    }\n",
    "    return sorted(links)\n",
    "\n",
    "def fetch_text(url):\n",
    "    \"\"\"Extract and return the full visible text from an article page.\"\"\"\n",
    "    r = session.get(url, timeout=(5, 15))\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    article = soup.find(\"article\") or soup\n",
    "    return article.get_text(separator=\"\\n\", strip=True)  #  [oai_citation_attribution:17‡Beautiful Soup Documentation](https://beautiful-soup-4.readthedocs.io/en/latest/?utm_source=chatgpt.com)\n",
    "\n",
    "def main():\n",
    "    os.makedirs(\"gc_texts/liahona\", exist_ok=True)\n",
    "\n",
    "    for yr, mo, month_url in month_urls():\n",
    "        ym_folder = f\"{yr}-{mo:02d}\"\n",
    "        out_dir = os.path.join(\"gc_texts/liahona\", ym_folder)\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "        print(f\"Processing {ym_folder}\")\n",
    "        for art_url in scrape_month(yr, mo, month_url):\n",
    "            slug = art_url.rstrip(\"/\").split(\"/\")[-1].replace(LANG_PARAM, \"\")\n",
    "            text = fetch_text(art_url)\n",
    "            local_path = os.path.join(out_dir, f\"{slug}.txt\")\n",
    "            with open(local_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(text)\n",
    "\n",
    "            # Upload to Google Drive\n",
    "            gfile = drive.CreateFile({'parents':[{'id': GDRIVE_FOLDER_ID}]})\n",
    "            gfile.SetContentFile(local_path)\n",
    "            gfile.Upload()\n",
    "            print(f\" → Uploaded {slug}.txt\")\n",
    "            time.sleep(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79381eb5-08b0-4f41-a11d-0b9a0ed54d79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn",
   "language": "python",
   "name": "cnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
