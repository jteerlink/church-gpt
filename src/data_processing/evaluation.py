"""
Evaluation Framework for LDS General Conference Fine-Tuning
Implements perplexity tracking, style similarity metrics, and training validation.
"""

import torch
import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional, Tuple
from collections import defaultdict, Counter
import re
import math
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.util import ngrams


class EvaluationMetrics:
    """Comprehensive evaluation metrics for fine-tuned models."""
    
    def __init__(self, tokenizer=None):
        """Initialize evaluation metrics with optional tokenizer."""
        self.tokenizer = tokenizer
        self._ensure_nltk_data()
    
    def _ensure_nltk_data(self):
        """Ensure required NLTK data is available."""
        try:
            nltk.data.find('tokenizers/punkt')
        except LookupError:
            nltk.download('punkt', quiet=True)
        
        try:
            nltk.data.find('tokenizers/punkt_tab')
        except LookupError:
            nltk.download('punkt_tab', quiet=True)
    
    def calculate_perplexity(self, model, eval_dataset, device='cpu') -> float:
        """
        Calculate perplexity on evaluation dataset.
        
        Args:
            model: The fine-tuned model
            eval_dataset: Evaluation dataset
            device: Device for computation
        
        Returns:
            Perplexity score
        """
        model.eval()
        total_loss = 0
        total_tokens = 0
        
        with torch.no_grad():
            for batch in eval_dataset:
                if isinstance(batch, dict):
                    input_ids = batch['input_ids'].to(device)
                    attention_mask = batch.get('attention_mask', None)
                    labels = batch.get('labels', input_ids)
                else:
                    input_ids = batch.to(device)
                    attention_mask = None
                    labels = input_ids
                
                outputs = model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )
                
                loss = outputs.loss
                total_loss += loss.item() * input_ids.size(0)
                total_tokens += input_ids.size(0)
        
        avg_loss = total_loss / total_tokens
        perplexity = math.exp(avg_loss)
        
        return perplexity
    
    def calculate_style_similarity(self, generated_text: str, 
                                 reference_corpus: List[str],
                                 author: str) -> Dict[str, float]:
        """
        Calculate style similarity metrics between generated and reference text.
        
        Args:
            generated_text: Text generated by the model
            reference_corpus: List of reference texts from the same author
            author: Author name for context
        
        Returns:
            Dictionary of style similarity metrics
        """
        metrics = {}
        
        # Vocabulary overlap analysis
        metrics['vocab_overlap'] = self._calculate_vocab_overlap(generated_text, reference_corpus)
        
        # Sentence structure similarity
        metrics['structure_similarity'] = self._analyze_sentence_structure(generated_text, reference_corpus)
        
        # Rhetorical pattern matching
        metrics['rhetorical_score'] = self._measure_rhetorical_patterns(generated_text, reference_corpus)
        
        # N-gram similarity
        metrics['ngram_similarity'] = self._calculate_ngram_similarity(generated_text, reference_corpus)
        
        # Text style features
        metrics['style_features'] = self._extract_style_features(generated_text, reference_corpus)
        
        # Overall similarity score (weighted combination)
        # Handle NaN values by replacing with 0
        safe_metrics = {}
        for key, value in metrics.items():
            if isinstance(value, float) and (np.isnan(value) or np.isinf(value)):
                safe_metrics[key] = 0.0
            else:
                safe_metrics[key] = value
        
        metrics.update(safe_metrics)
        
        metrics['overall_similarity'] = (
            metrics['vocab_overlap'] * 0.25 +
            metrics['structure_similarity'] * 0.25 +
            metrics['rhetorical_score'] * 0.20 +
            metrics['ngram_similarity'] * 0.20 +
            metrics['style_features'] * 0.10
        )
        
        return metrics
    
    def _calculate_vocab_overlap(self, generated_text: str, reference_corpus: List[str]) -> float:
        """Calculate vocabulary overlap between generated and reference text."""
        generated_words = set(word_tokenize(generated_text.lower()))
        
        reference_words = set()
        for text in reference_corpus:
            reference_words.update(word_tokenize(text.lower()))
        
        if not reference_words:
            return 0.0
        
        overlap = len(generated_words.intersection(reference_words))
        return overlap / len(reference_words) if reference_words else 0.0
    
    def _analyze_sentence_structure(self, generated_text: str, reference_corpus: List[str]) -> float:
        """Analyze sentence structure similarity."""
        def get_sentence_features(text):
            sentences = sent_tokenize(text)
            if not sentences:
                return {'avg_length': 0, 'length_variance': 0}
            
            lengths = [len(word_tokenize(sent)) for sent in sentences]
            return {
                'avg_length': np.mean(lengths),
                'length_variance': np.var(lengths) if len(lengths) > 1 else 0
            }
        
        generated_features = get_sentence_features(generated_text)
        
        reference_features_list = [get_sentence_features(text) for text in reference_corpus]
        avg_ref_length = np.mean([f['avg_length'] for f in reference_features_list])
        avg_ref_variance = np.mean([f['length_variance'] for f in reference_features_list])
        
        # Calculate similarity based on sentence length patterns
        length_similarity = 1 - abs(generated_features['avg_length'] - avg_ref_length) / max(avg_ref_length, 1)
        variance_similarity = 1 - abs(generated_features['length_variance'] - avg_ref_variance) / max(avg_ref_variance, 1)
        
        return (length_similarity + variance_similarity) / 2
    
    def _measure_rhetorical_patterns(self, generated_text: str, reference_corpus: List[str]) -> float:
        """Measure rhetorical pattern similarity."""
        # Common LDS conference rhetorical patterns
        patterns = [
            r'brothers and sisters',
            r'i (know|testify|witness) that',
            r'let me (share|tell|suggest)',
            r'my dear (friends|brothers|sisters)',
            r'i invite you to',
            r'may (we|you|i)',
            r'(father|lord|god) (has|will|wants)',
            r'in the name of jesus christ'
        ]
        
        def count_patterns(text):
            text_lower = text.lower()
            pattern_counts = {}
            for pattern in patterns:
                matches = len(re.findall(pattern, text_lower))
                pattern_counts[pattern] = matches
            return pattern_counts
        
        generated_patterns = count_patterns(generated_text)
        
        # Calculate pattern frequency in reference corpus
        reference_patterns = defaultdict(list)
        for text in reference_corpus:
            text_patterns = count_patterns(text)
            for pattern, count in text_patterns.items():
                reference_patterns[pattern].append(count)
        
        # Calculate similarity for each pattern
        pattern_similarities = []
        for pattern in patterns:
            gen_count = generated_patterns.get(pattern, 0)
            ref_counts = reference_patterns.get(pattern, [0])
            avg_ref_count = np.mean(ref_counts)
            
            if avg_ref_count > 0:
                similarity = min(gen_count / avg_ref_count, 2.0) / 2.0  # Cap at 1.0
            else:
                similarity = 1.0 if gen_count == 0 else 0.0
            
            pattern_similarities.append(similarity)
        
        return np.mean(pattern_similarities)
    
    def _calculate_ngram_similarity(self, generated_text: str, reference_corpus: List[str]) -> float:
        """Calculate n-gram similarity using TF-IDF."""
        if not reference_corpus:
            return 0.0
        
        # Combine reference corpus
        reference_text = ' '.join(reference_corpus)
        
        # Create TF-IDF vectors for bigrams and trigrams
        vectorizer = TfidfVectorizer(ngram_range=(2, 3), max_features=1000)
        
        try:
            tfidf_matrix = vectorizer.fit_transform([reference_text, generated_text])
            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
            return similarity
        except ValueError:
            return 0.0
    
    def _extract_style_features(self, generated_text: str, reference_corpus: List[str]) -> float:
        """Extract and compare style features."""
        def extract_features(text):
            words = word_tokenize(text)
            sentences = sent_tokenize(text)
            
            if not words or not sentences:
                return {}
            
            return {
                'avg_word_length': np.mean([len(word) for word in words]),
                'type_token_ratio': len(set(words)) / len(words),
                'avg_sentence_length': len(words) / len(sentences),
                'punctuation_density': len([c for c in text if c in '.,!?;:']) / len(text)
            }
        
        generated_features = extract_features(generated_text)
        
        if not reference_corpus:
            return 0.0
        
        reference_features_list = [extract_features(text) for text in reference_corpus]
        
        # Calculate average reference features
        avg_ref_features = {}
        for key in generated_features:
            values = [f.get(key, 0) for f in reference_features_list if f.get(key) is not None]
            avg_ref_features[key] = np.mean(values) if values else 0
        
        # Calculate similarity for each feature
        similarities = []
        for key in generated_features:
            gen_val = generated_features[key]
            ref_val = avg_ref_features.get(key, 0)
            
            if ref_val > 0:
                similarity = 1 - abs(gen_val - ref_val) / ref_val
                similarities.append(max(0, similarity))
        
        return np.mean(similarities) if similarities else 0.0


class ValidationSetCreator:
    """Creates stratified validation sets maintaining author representation."""
    
    def __init__(self, test_ratio: float = 0.15, random_state: int = 42):
        """
        Initialize validation set creator.
        
        Args:
            test_ratio: Proportion of data for validation
            random_state: Random seed for reproducibility
        """
        self.test_ratio = test_ratio
        self.random_state = random_state
    
    def create_validation_split(self, documents: List[Dict[str, Any]]) -> Tuple[List[Dict], List[Dict]]:
        """
        Create stratified train/validation split maintaining author representation.
        
        Args:
            documents: List of documents with author metadata
        
        Returns:
            Tuple of (train_documents, validation_documents)
        """
        # Group documents by author
        author_groups = defaultdict(list)
        for doc in documents:
            author = doc.get('author', 'Unknown')
            author_groups[author].append(doc)
        
        train_docs = []
        val_docs = []
        
        # Split each author's documents proportionally
        for author, author_docs in author_groups.items():
            if len(author_docs) == 1:
                # Single document: goes to training
                train_docs.extend(author_docs)
            else:
                # Multiple documents: stratified split
                train_split, val_split = train_test_split(
                    author_docs,
                    test_size=self.test_ratio,
                    random_state=self.random_state
                )
                train_docs.extend(train_split)
                val_docs.extend(val_split)
        
        return train_docs, val_docs
    
    def analyze_split_quality(self, train_docs: List[Dict], val_docs: List[Dict]) -> Dict[str, Any]:
        """
        Analyze quality of train/validation split.
        
        Args:
            train_docs: Training documents
            val_docs: Validation documents
        
        Returns:
            Analysis report of split quality
        """
        def get_author_distribution(docs):
            authors = [doc.get('author', 'Unknown') for doc in docs]
            return Counter(authors)
        
        train_authors = get_author_distribution(train_docs)
        val_authors = get_author_distribution(val_docs)
        
        all_authors = set(train_authors.keys()) | set(val_authors.keys())
        
        # Calculate representation balance
        author_balance = {}
        for author in all_authors:
            train_count = train_authors.get(author, 0)
            val_count = val_authors.get(author, 0)
            total_count = train_count + val_count
            
            if total_count > 0:
                val_ratio = val_count / total_count
                author_balance[author] = {
                    'train_count': train_count,
                    'val_count': val_count,
                    'val_ratio': val_ratio
                }
        
        # Overall statistics
        total_docs = len(train_docs) + len(val_docs)
        actual_val_ratio = len(val_docs) / total_docs if total_docs > 0 else 0
        
        return {
            'split_summary': {
                'total_documents': total_docs,
                'train_documents': len(train_docs),
                'validation_documents': len(val_docs),
                'actual_validation_ratio': actual_val_ratio,
                'target_validation_ratio': self.test_ratio
            },
            'author_representation': author_balance,
            'author_coverage': {
                'total_authors': len(all_authors),
                'authors_in_train': len(train_authors),
                'authors_in_validation': len(val_authors),
                'authors_in_both': len(set(train_authors.keys()) & set(val_authors.keys()))
            }
        }


class EarlyStoppingCallback:
    """Early stopping implementation to prevent overfitting."""
    
    def __init__(self, patience: int = 3, min_delta: float = 0.01, 
                 metric: str = 'eval_loss', mode: str = 'min'):
        """
        Initialize early stopping callback.
        
        Args:
            patience: Number of evaluations to wait before stopping
            min_delta: Minimum change to qualify as improvement
            metric: Metric to monitor for improvement
            mode: 'min' for minimizing metric, 'max' for maximizing
        """
        self.patience = patience
        self.min_delta = min_delta
        self.metric = metric
        self.mode = mode
        self.best_value = float('inf') if mode == 'min' else float('-inf')
        self.wait_count = 0
        self.stopped_epoch = 0
        self.should_stop = False
    
    def __call__(self, logs: Dict[str, float], epoch: int) -> bool:
        """
        Check if training should stop based on metric improvement.
        
        Args:
            logs: Dictionary of metric values
            epoch: Current epoch number
        
        Returns:
            True if training should stop, False otherwise
        """
        current_value = logs.get(self.metric)
        if current_value is None:
            return False
        
        # First evaluation - set baseline
        if self.best_value == float('inf') and self.mode == 'min':
            self.best_value = current_value
            return False
        elif self.best_value == float('-inf') and self.mode == 'max':
            self.best_value = current_value
            return False
        
        # Check if improvement is significant enough to reset patience
        previous_best = self.best_value
        
        if self.mode == 'min':
            is_significant_improvement = (previous_best - current_value) > self.min_delta
        else:
            is_significant_improvement = (current_value - previous_best) > self.min_delta
        
        # Update best value if there's any improvement
        if self.mode == 'min' and current_value < self.best_value:
            self.best_value = current_value
        elif self.mode == 'max' and current_value > self.best_value:
            self.best_value = current_value
        
        if is_significant_improvement:
            self.wait_count = 0
        else:
            self.wait_count += 1
        
        if self.wait_count >= self.patience:
            self.should_stop = True
            self.stopped_epoch = epoch
            return True
        
        return False
    
    def get_best_value(self) -> float:
        """Get the best metric value observed."""
        return self.best_value
    
    def reset(self):
        """Reset callback state."""
        self.best_value = float('inf') if self.mode == 'min' else float('-inf')
        self.wait_count = 0
        self.stopped_epoch = 0
        self.should_stop = False


class TrainingEvaluator:
    """Comprehensive training evaluation and monitoring system."""
    
    def __init__(self, validation_ratio: float = 0.15):
        """
        Initialize training evaluator.
        
        Args:
            validation_ratio: Proportion of data for validation
        """
        self.validation_ratio = validation_ratio
        self.metrics_calculator = EvaluationMetrics()
        self.split_creator = ValidationSetCreator(test_ratio=validation_ratio)
        self.training_history = []
        self.author_corpuses = defaultdict(list)
    
    def prepare_evaluation_datasets(self, training_examples: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Prepare training and validation datasets with author stratification.
        
        Args:
            training_examples: List of training examples with metadata
        
        Returns:
            Dictionary containing train/val splits and analysis
        """
        # Group examples by author for corpus building
        for example in training_examples:
            author = example['metadata']['author']
            self.author_corpuses[author].append(example['response'])
        
        # Create stratified split
        train_examples, val_examples = self.split_creator.create_validation_split(
            [{'author': ex['metadata']['author'], **ex} for ex in training_examples]
        )
        
        # Analyze split quality
        split_analysis = self.split_creator.analyze_split_quality(train_examples, val_examples)
        
        return {
            'train_examples': train_examples,
            'validation_examples': val_examples,
            'split_analysis': split_analysis,
            'author_corpuses': dict(self.author_corpuses)
        }
    
    def evaluate_model_output(self, generated_text: str, author: str, 
                            reference_examples: List[str]) -> Dict[str, Any]:
        """
        Evaluate generated text against author's style and reference examples.
        
        Args:
            generated_text: Text generated by model
            author: Expected author
            reference_examples: Reference texts from same author
        
        Returns:
            Comprehensive evaluation results
        """
        # Style similarity analysis
        style_metrics = self.metrics_calculator.calculate_style_similarity(
            generated_text, reference_examples, author
        )
        
        # Content quality analysis
        quality_metrics = self._analyze_content_quality(generated_text)
        
        # Author consistency check
        consistency_metrics = self._check_author_consistency(generated_text, author, reference_examples)
        
        return {
            'style_similarity': style_metrics,
            'content_quality': quality_metrics,
            'author_consistency': consistency_metrics,
            'overall_score': (
                style_metrics['overall_similarity'] * 0.5 +
                quality_metrics['overall_quality'] * 0.3 +
                consistency_metrics['consistency_score'] * 0.2
            )
        }
    
    def _analyze_content_quality(self, text: str) -> Dict[str, float]:
        """Analyze general content quality metrics."""
        words = word_tokenize(text)
        sentences = sent_tokenize(text)
        
        if not words or not sentences:
            return {'overall_quality': 0.0}
        
        metrics = {
            'readability': self._calculate_readability_score(text),
            'coherence': self._measure_text_coherence(text),
            'completeness': self._assess_content_completeness(text),
            'grammar_quality': self._assess_grammar_quality(text)
        }
        
        # Overall quality score
        metrics['overall_quality'] = np.mean(list(metrics.values()))
        
        return metrics
    
    def _calculate_readability_score(self, text: str) -> float:
        """Calculate simple readability score."""
        words = word_tokenize(text)
        sentences = sent_tokenize(text)
        
        if not sentences:
            return 0.0
        
        avg_sentence_length = len(words) / len(sentences)
        avg_word_length = np.mean([len(word) for word in words])
        
        # Simple readability approximation (lower is more readable)
        readability = 1 / (1 + (avg_sentence_length / 20) + (avg_word_length / 6))
        return readability
    
    def _measure_text_coherence(self, text: str) -> float:
        """Measure text coherence through sentence connectivity."""
        sentences = sent_tokenize(text)
        if len(sentences) < 2:
            return 1.0
        
        # Simple coherence measure based on word overlap between adjacent sentences
        coherence_scores = []
        for i in range(len(sentences) - 1):
            words1 = set(word_tokenize(sentences[i].lower()))
            words2 = set(word_tokenize(sentences[i + 1].lower()))
            
            overlap = len(words1.intersection(words2))
            total_unique = len(words1.union(words2))
            
            if total_unique > 0:
                coherence_scores.append(overlap / total_unique)
        
        return np.mean(coherence_scores) if coherence_scores else 0.0
    
    def _assess_content_completeness(self, text: str) -> float:
        """Assess whether content appears complete and well-formed."""
        # Check for proper ending
        has_proper_ending = text.strip().endswith(('.', '!', '?'))
        
        # Check for reasonable length
        word_count = len(word_tokenize(text))
        length_score = min(word_count / 50, 1.0)  # Normalize around 50 words
        
        # Check for sentence structure
        sentences = sent_tokenize(text)
        has_multiple_sentences = len(sentences) >= 2
        
        completeness_score = (
            (1.0 if has_proper_ending else 0.5) * 0.4 +
            length_score * 0.4 +
            (1.0 if has_multiple_sentences else 0.7) * 0.2
        )
        
        return completeness_score
    
    def _assess_grammar_quality(self, text: str) -> float:
        """Simple grammar quality assessment."""
        # Basic grammar indicators
        has_capitalization = any(c.isupper() for c in text)
        has_punctuation = any(c in '.,!?;:' for c in text)
        
        # Check for common grammar patterns
        sentences = sent_tokenize(text)
        properly_capitalized = sum(1 for sent in sentences if sent and sent[0].isupper())
        capitalization_ratio = properly_capitalized / len(sentences) if sentences else 0
        
        grammar_score = (
            (1.0 if has_capitalization else 0.0) * 0.3 +
            (1.0 if has_punctuation else 0.0) * 0.3 +
            capitalization_ratio * 0.4
        )
        
        return grammar_score
    
    def _check_author_consistency(self, generated_text: str, expected_author: str, 
                                reference_examples: List[str]) -> Dict[str, float]:
        """Check consistency with expected author's style."""
        if not reference_examples:
            return {'consistency_score': 0.5}  # Neutral score without reference
        
        # Calculate style similarity with this author's corpus
        author_similarity = self.metrics_calculator.calculate_style_similarity(
            generated_text, reference_examples, expected_author
        )
        
        return {
            'consistency_score': author_similarity['overall_similarity'],
            'vocab_consistency': author_similarity['vocab_overlap'],
            'structural_consistency': author_similarity['structure_similarity'],
            'rhetorical_consistency': author_similarity['rhetorical_score']
        }
    
    def log_training_metrics(self, epoch: int, metrics: Dict[str, float]):
        """Log training metrics for analysis."""
        log_entry = {
            'epoch': epoch,
            'timestamp': pd.Timestamp.now(),
            **metrics
        }
        self.training_history.append(log_entry)
    
    def generate_evaluation_report(self, model_outputs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Generate comprehensive evaluation report.
        
        Args:
            model_outputs: List of model evaluation results
        
        Returns:
            Comprehensive evaluation report
        """
        if not model_outputs:
            return {'error': 'No model outputs provided'}
        
        # Aggregate metrics
        all_style_scores = [output['style_similarity']['overall_similarity'] for output in model_outputs]
        all_quality_scores = [output['content_quality']['overall_quality'] for output in model_outputs]
        all_consistency_scores = [output['author_consistency']['consistency_score'] for output in model_outputs]
        all_overall_scores = [output['overall_score'] for output in model_outputs]
        
        # Author-specific analysis
        author_performance = defaultdict(list)
        for output in model_outputs:
            # Assuming author info is available in output context
            author = output.get('author', 'Unknown')
            author_performance[author].append(output['overall_score'])
        
        report = {
            'summary_metrics': {
                'avg_style_similarity': np.mean(all_style_scores),
                'avg_content_quality': np.mean(all_quality_scores),
                'avg_author_consistency': np.mean(all_consistency_scores),
                'avg_overall_score': np.mean(all_overall_scores),
                'total_evaluations': len(model_outputs)
            },
            'performance_distribution': {
                'style_similarity_std': np.std(all_style_scores),
                'content_quality_std': np.std(all_quality_scores),
                'consistency_std': np.std(all_consistency_scores),
                'score_range': {
                    'min_score': min(all_overall_scores),
                    'max_score': max(all_overall_scores),
                    'score_span': max(all_overall_scores) - min(all_overall_scores)
                }
            },
            'author_analysis': {
                author: {
                    'avg_score': np.mean(scores),
                    'score_std': np.std(scores),
                    'sample_count': len(scores)
                }
                for author, scores in author_performance.items()
            },
            'quality_thresholds': {
                'excellent': sum(1 for score in all_overall_scores if score >= 0.8),
                'good': sum(1 for score in all_overall_scores if 0.6 <= score < 0.8),
                'acceptable': sum(1 for score in all_overall_scores if 0.4 <= score < 0.6),
                'poor': sum(1 for score in all_overall_scores if score < 0.4)
            }
        }
        
        return report


def compute_training_metrics(eval_predictions) -> Dict[str, float]:
    """
    Compute metrics for training evaluation.
    Compatible with HuggingFace Trainer.
    
    Args:
        eval_predictions: Predictions and labels from evaluation
    
    Returns:
        Dictionary of computed metrics
    """
    predictions, labels = eval_predictions
    
    # Calculate perplexity from loss
    if hasattr(predictions, 'loss'):
        loss = predictions.loss
    else:
        # Calculate cross-entropy loss manually if needed
        loss = torch.nn.functional.cross_entropy(
            torch.tensor(predictions).view(-1, predictions.shape[-1]),
            torch.tensor(labels).view(-1),
            ignore_index=-100
        ).item()
    
    perplexity = math.exp(loss)
    
    return {
        'perplexity': perplexity,
        'eval_loss': loss
    }


def create_evaluation_framework(validation_ratio: float = 0.15, 
                              early_stopping_patience: int = 3) -> Dict[str, Any]:
    """
    Factory function to create complete evaluation framework.
    
    Args:
        validation_ratio: Proportion of data for validation
        early_stopping_patience: Patience for early stopping
    
    Returns:
        Dictionary containing all evaluation components
    """
    evaluator = TrainingEvaluator(validation_ratio)
    early_stopping = EarlyStoppingCallback(patience=early_stopping_patience)
    
    return {
        'evaluator': evaluator,
        'early_stopping': early_stopping,
        'metrics_calculator': evaluator.metrics_calculator,
        'split_creator': evaluator.split_creator,
        'compute_metrics_fn': compute_training_metrics
    }